  Double your experiments, double your inventiveness.

== IMDB review pos/neg prediction ==

==== Reference ====
https://livebook.manning.com/book/grokking-deep-learning/chapter-11/[Grokking Deep Learning: Chapter 11. Neural networks that understand language]

==== Data set ====
http://ai.stanford.edu/~amaas/data/sentiment/[Large Movie Review Dataset]

==== Method ====
https://github.com/dhkim9549/ai-study/blob/main/imdb/train-review.py[train-review.py]

Build a simple nn using numpy

Make a list of vocabulary from the data set

Train

* Using the vocab list, an input text is encoded into vectors using one-hot encoding.

Result

* 80% acuuracy achieved

----
# python3 train-review.py
crctRat = 0.0
crctRat = 0.521
crctRat = 0.554
crctRat = 0.586
crctRat = 0.555
crctRat = 0.588
crctRat = 0.557

...

crctRat = 0.809
crctRat = 0.798
crctRat = 0.822
crctRat = 0.828
crctRat = 0.796
crctRat = 0.817
crctRat = 0.82
----

* As the length of the input vector increases, the training speed slows down significantly. When the lenth of the input vector was larger than 10000, the training was too slow.

==== Method 2 ====

Build a nn using PyTorch

* Will this be faster than pure numpy?

Indeed, the python program using PyTorch acheives near perfrect result with much faster training. It seems the nn is overfit to the training data.

----
# nohup python3 -u train-review-torch.py > train-review-torch.log &

cnt = 0
crctRat = 0.0
loss = 0.28654032945632935
y = tensor([[0.5353, 0.4647]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/12237_2.txt

cnt = 1000
crctRat = 0.544
loss = 0.1975749433040619
y = tensor([[0.4445, 0.5555]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/12085_3.txt

cnt = 2000
crctRat = 0.647
loss = 0.2767987847328186
y = tensor([[0.4739, 0.5261]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[1., 0.]])
reviewFile = /root/data/aclImdb/train/pos/11121_10.txt

cnt = 3000
crctRat = 0.751
loss = 0.1967540979385376
y = tensor([[0.4436, 0.5564]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/7855_1.txt

...

cnt = 631000
crctRat = 0.981
loss = 0.0012777902884408832
y = tensor([[0.0357, 0.9643]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/10299_4.txt

cnt = 632000
crctRat = 0.981
loss = 2.4416371502411494e-07
y = tensor([[4.9414e-04, 9.9951e-01]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/1242_1.txt

cnt = 633000
crctRat = 0.991
loss = 7.489006748073734e-09
y = tensor([[8.6532e-05, 9.9991e-01]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/5538_4.txt

----

==== Evaluation ====
https://github.com/dhkim9549/ai-study/blob/main/imdb/train-review-torch-eval.py[train-review-torch-eval.py]

Using test data set, model evaluation was done.
It achieved 86% success rate.

----

cnt = 0
crctRat = 1.0
loss = 1.4476287567485935e-19
y = tensor([[1.0000e+00, 5.3808e-10]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[1., 0.]])
reviewFile = /root/data/aclImdb/test/pos/12197_7.txt

cnt = 1000
crctRat = 0.863
loss = 1.073484579900601e-14
y = tensor([[1.0000e+00, 8.5199e-08]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[1., 0.]])
reviewFile = /root/data/aclImdb/test/pos/7820_7.txt

cnt = 2000
crctRat = 0.841
loss = 0.9999931454658508
y = tensor([[1.0000e+00, 3.4086e-06]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/test/neg/3985_4.txt

cnt = 3000
crctRat = 0.867
loss = 1.0947590056709089e-11
y = tensor([[3.2793e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/test/neg/3728_1.txt

----

==== Serving nn as a REST API ====

https://github.com/dhkim9549/ai-study/blob/main/imdb/app.py[app.py]

  $ flask run --host=0.0.0.0

The nn is served using Flask.

https://flask.palletsprojects.com/en/2.2.x/quickstart/[Flask Quickstart]

http://bada.ai/ai/sentiment-analysis.html[Sentiment Analysis web page]

The actual web page for the test.

==== Discussions ====
Relations among words cannot be considered with one-hot encoding.
To consider the relations among words, attention mechanism is necessary.

https://arxiv.org/abs/1706.03762[Attention is all you need.]

==== Dropout ====

Perform dropout to see better results can be acheived with the test data set.

With dropout, slightly better result was acheived.

----
cnt = 88000
crctRat = 0.8713310076021863
loss = 0.0003201336367055774
y = tensor([[0.0179, 0.9821]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/test/neg/254_3.txt

cnt = 89000
crctRat = 0.8710126852507275
loss = 0.3714914321899414
y = tensor([[0.6095, 0.3905]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/test/neg/6337_1.txt

cnt = 90000
crctRat = 0.8709236564038177
loss = 0.01285428274422884
y = tensor([[0.1134, 0.8866]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])

----
