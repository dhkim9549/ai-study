== IMDB review pos/neg prediction ==

==== Reference ====
https://livebook.manning.com/book/grokking-deep-learning/chapter-11/[Grokking Deep Learning: Chapter 11. Neural networks that understand language]

==== Data set ====
http://ai.stanford.edu/~amaas/data/sentiment/[Large Movie Review Dataset]

==== Method ====
https://github.com/dhkim9549/ai-study/blob/main/imdb/train-review.py[train-review.py]

Build a simple nn using numpy

Make a list of vocabulary from the data set

* Truncated the length of the vocab list 

Train

* Using the vocab list, an input text is encoded into vectors usnig one-hot encoding.

Result

* 80% acuuracy achieved

----
# python3 train-review.py
crctRat = 0.0
crctRat = 0.521
crctRat = 0.554
crctRat = 0.586
crctRat = 0.555
crctRat = 0.588
crctRat = 0.557

...

crctRat = 0.809
crctRat = 0.798
crctRat = 0.822
crctRat = 0.828
crctRat = 0.796
crctRat = 0.817
crctRat = 0.82
----

* As the length of the input vector increases, the training speed slows down significantly. When the lenth of the input vector was larger than 10000, the training was too slow.

==== Method 2 ====

Build a nn using PyTorch

* Will this be faster than pure numpy?

Indeed, the python program using PyTorch acheives near perfrect result with much faster training!

----
# nohup python3 -u train-review-torch.py > train-review-torch.log &

cnt = 0
crctRat = 0.0
loss = 0.28654032945632935
y = tensor([[0.5353, 0.4647]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/12237_2.txt

cnt = 1000
crctRat = 0.544
loss = 0.1975749433040619
y = tensor([[0.4445, 0.5555]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/12085_3.txt

cnt = 2000
crctRat = 0.647
loss = 0.2767987847328186
y = tensor([[0.4739, 0.5261]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[1., 0.]])
reviewFile = /root/data/aclImdb/train/pos/11121_10.txt

cnt = 3000
crctRat = 0.751
loss = 0.1967540979385376
y = tensor([[0.4436, 0.5564]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/7855_1.txt

...

cnt = 631000
crctRat = 0.981
loss = 0.0012777902884408832
y = tensor([[0.0357, 0.9643]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/10299_4.txt

cnt = 632000
crctRat = 0.981
loss = 2.4416371502411494e-07
y = tensor([[4.9414e-04, 9.9951e-01]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/1242_1.txt

cnt = 633000
crctRat = 0.991
loss = 7.489006748073734e-09
y = tensor([[8.6532e-05, 9.9991e-01]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/train/neg/5538_4.txt

----

==== Evaluation ====
https://github.com/dhkim9549/ai-study/blob/main/imdb/train-review-torch-eval.py[train-review-torch-eval.py]

Using test data set, model evaluation was done.
It achieved 86% success rate.

----

cnt = 0
crctRat = 1.0
loss = 1.4476287567485935e-19
y = tensor([[1.0000e+00, 5.3808e-10]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[1., 0.]])
reviewFile = /root/data/aclImdb/test/pos/12197_7.txt

cnt = 1000
crctRat = 0.863
loss = 1.073484579900601e-14
y = tensor([[1.0000e+00, 8.5199e-08]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[1., 0.]])
reviewFile = /root/data/aclImdb/test/pos/7820_7.txt

cnt = 2000
crctRat = 0.841
loss = 0.9999931454658508
y = tensor([[1.0000e+00, 3.4086e-06]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/test/neg/3985_4.txt

cnt = 3000
crctRat = 0.867
loss = 1.0947590056709089e-11
y = tensor([[3.2793e-06, 1.0000e+00]], grad_fn=<SoftmaxBackward0>)
y0 = tensor([[0., 1.]])
reviewFile = /root/data/aclImdb/test/neg/3728_1.txt

----
